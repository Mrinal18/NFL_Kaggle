{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "    \"\"\" \n",
    "    https://stackoverflow.com/questions/26646362/numpy-array-is-not-json-serializable\n",
    "    Special json encoder for numpy types\n",
    "    \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "class COCOConverter:\n",
    "    \"\"\"Class to convert competition csv to coco format.\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame, \n",
    "        image_height: int = 720, \n",
    "        image_width: int = 1280, \n",
    "        type_agnostic: bool = False):\n",
    "        \n",
    "        self.image_height = image_height\n",
    "        self.image_width = image_width\n",
    "        self.type_agnostic = type_agnostic\n",
    "        if self.type_agnostic:\n",
    "            self.categories = [{\"id\": 1, \"name\": \"Helmet\"}]\n",
    "        else:\n",
    "            self.categories = [\n",
    "                {\"id\": 1, \"name\": \"impact_None\",},\n",
    "                {\"id\": 2, \"name\": \"impact_Helmet\"},\n",
    "                {\"id\": 3, \"name\": \"impact_Shoulder\",},\n",
    "                {\"id\": 4, \"name\": \"impact_Body\"},\n",
    "                {\"id\": 5, \"name\": \"impact_Ground\",},\n",
    "                {\"id\": 6, \"name\": \"impact_Hand\"},\n",
    "            ]         \n",
    "        self.df = self._initialize(df)\n",
    "\n",
    "    def _get_file_name(self, row: pd.Series):\n",
    "        base_name = row.video[:-4]\n",
    "        file_name = f'{base_name}_frame{row.frame:04}.jpg'\n",
    "        return file_name\n",
    "\n",
    "    def _get_bbox(self, row: pd.Series):\n",
    "        return [row.left, row.top, row.width, row.height]\n",
    "\n",
    "    def _initialize(self, df: pd.DataFrame):\n",
    "        # set category id\n",
    "        if self.type_agnostic:\n",
    "            df['impactType'] = 'Helmet'\n",
    "            df['category_id'] = 1\n",
    "        else:\n",
    "            df['category_id'] = df['impactType'].map(\n",
    "                {\n",
    "                    'None': 1,\n",
    "                    'Helmet': 2,\n",
    "                    'Shoulder': 3,\n",
    "                    'Body': 4,\n",
    "                    'Ground': 5,\n",
    "                    'Hand': 6\n",
    "                }\n",
    "            )\n",
    "        # some preprocesses\n",
    "        df['file_name'] = df[['video', 'frame']].progress_apply(self._get_file_name, axis=1)\n",
    "        df['area'] = df['width'] * df['height']\n",
    "        df['bbox'] = df[['left', 'top', 'height', 'width']].progress_apply(self._get_bbox, axis=1)\n",
    "        df['iscrowd'] = 0\n",
    "        return df\n",
    "        \n",
    "\n",
    "    def save(self, save_path):\n",
    "        \"\"\"\n",
    "        Save as coco json format.\n",
    "        But also has many supplemental items like gameKey or view.\n",
    "        \"\"\"\n",
    "        df = self.df.copy()\n",
    "        image_df = df[['gameKey', 'playID', 'view', 'video', 'frame', 'file_name']].drop_duplicates()\n",
    "        image_df['height'] = self.image_height\n",
    "        image_df['width'] = self.image_width\n",
    "        \n",
    "        # add image id to images. Note that it's called just \"id\".\n",
    "        image_df['id'] = range(1, len(image_df) + 1)\n",
    "    \n",
    "        # add image id to annotations.\n",
    "        df['image_id'] = df[['file_name']].merge(image_df[['file_name', 'id']])['id'].values\n",
    "        df['id'] = range(1, len(df) + 1)\n",
    "\n",
    "        print('start dumping...')\n",
    "        coco_annotations = dict()\n",
    "        coco_annotations['categories'] = self.categories\n",
    "        coco_annotations['images'] = [dict(row) for _, row in image_df.iterrows()]\n",
    "        coco_annotations['annotations'] = [dict(row) for _, row in df.iterrows()]\n",
    "        json.dump(coco_annotations, open(save_path, 'w'), indent=4, cls=NumpyEncoder)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('../input/nfl-health-and-safety-helmet-assignment/train_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_ids = df['playID'].unique()\n",
    "num_train = int(len(play_ids) * 0.8)\n",
    "train_play_ids = df['playID'].unique()[:num_train]\n",
    "valid_play_ids = df['playID'].unique()[num_train:]\n",
    "print('number of train videos:', len(train_play_ids))\n",
    "print('number of valid videos:', len(valid_play_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.query('playID in @train_play_ids').reset_index(drop=True).copy()\n",
    "valid_df = df.query('playID in @valid_play_ids').reset_index(drop=True).copy()\n",
    "\n",
    "print('number of train annotations:', len(train_df))\n",
    "print('number of valid annotations:', len(valid_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "train_coco = COCOConverter(train_df, type_agnostic=True)\n",
    "train_coco.save('train.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "valid_coco = COCOConverter(valid_df, type_agnostic=True)\n",
    "valid_coco.save('valid.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "from tqdm.auto import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "def split_to_images(video_path):\n",
    "    video_name = os.path.basename(video_path).split('.')[0]\n",
    "    cam = cv2.VideoCapture(video_path)\n",
    "    frame_count = 1 # To make it consistant with train_labels.csv\n",
    "    while True:\n",
    "        successed, img = cam.read()\n",
    "        if not successed:\n",
    "            break\n",
    "        save_name = f'{SAVE_DIR}/{video_name}_frame{frame_count:04}.jpg'\n",
    "        cv2.imwrite(save_name, img)\n",
    "        frame_count += 1\n",
    "    print(f'finished processing {video_name}')\n",
    "    \n",
    "if __name__ == '__main__': \n",
    "    SAVE_DIR = 'train_images'\n",
    "    os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "    video_paths = sorted(\n",
    "        glob.glob('../input/nfl-health-and-safety-helmet-assignment/train/*'))\n",
    "\n",
    "    num_cpu = cpu_count()\n",
    "    pool = Pool(num_cpu)\n",
    "    with tqdm(total=len(video_paths)) as t:\n",
    "        for _ in pool.imap_unordered(split_to_images, video_paths):\n",
    "            t.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/facebookresearch/detectron2.git\n",
    "%cd detectron2\n",
    "!python -m pip install -e ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile detectron2/data/datasets/builtin.py\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "# Copyright (c) Facebook, Inc. and its affiliates.\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "This file registers pre-defined datasets at hard-coded paths, and their metadata.\n",
    "\n",
    "We hard-code metadata for common datasets. This will enable:\n",
    "1. Consistency check when loading the datasets\n",
    "2. Use models on these standard datasets directly and run demos,\n",
    "   without having to download the dataset annotations\n",
    "\n",
    "We hard-code some paths to the dataset that's assumed to\n",
    "exist in \"./datasets/\".\n",
    "\n",
    "Users SHOULD NOT use this file to create new dataset / metadata for new dataset.\n",
    "To add new dataset, refer to the tutorial \"docs/DATASETS.md\".\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "from .builtin_meta import ADE20K_SEM_SEG_CATEGORIES, _get_builtin_metadata\n",
    "from .cityscapes import load_cityscapes_instances, load_cityscapes_semantic\n",
    "from .cityscapes_panoptic import register_all_cityscapes_panoptic\n",
    "from .coco import load_sem_seg, register_coco_instances\n",
    "from .coco_panoptic import register_coco_panoptic, register_coco_panoptic_separated\n",
    "from .lvis import get_lvis_instances_meta, register_lvis_instances\n",
    "from .pascal_voc import register_pascal_voc\n",
    "\n",
    "# ==== Predefined datasets and splits for COCO ==========\n",
    "\n",
    "_PREDEFINED_SPLITS_COCO = {}\n",
    "_PREDEFINED_SPLITS_COCO[\"coco\"] = {\n",
    "    \"coco_2014_train\": (\"coco/train2014\", \"coco/annotations/instances_train2014.json\"),\n",
    "    \"coco_2014_val\": (\"coco/val2014\", \"coco/annotations/instances_val2014.json\"),\n",
    "    \"coco_2014_minival\": (\"coco/val2014\", \"coco/annotations/instances_minival2014.json\"),\n",
    "    \"coco_2014_minival_100\": (\"coco/val2014\", \"coco/annotations/instances_minival2014_100.json\"),\n",
    "    \"coco_2014_valminusminival\": (\n",
    "        \"coco/val2014\",\n",
    "        \"coco/annotations/instances_valminusminival2014.json\",\n",
    "    ),\n",
    "    \"coco_2017_train\": (\"coco/train2017\", \"coco/annotations/instances_train2017.json\"),\n",
    "    \"coco_2017_val\": (\"coco/val2017\", \"coco/annotations/instances_val2017.json\"),\n",
    "    \"coco_2017_test\": (\"coco/test2017\", \"coco/annotations/image_info_test2017.json\"),\n",
    "    \"coco_2017_test-dev\": (\"coco/test2017\", \"coco/annotations/image_info_test-dev2017.json\"),\n",
    "    \"coco_2017_val_100\": (\"coco/val2017\", \"coco/annotations/instances_val2017_100.json\"),\n",
    "}\n",
    "\n",
    "_PREDEFINED_SPLITS_COCO[\"coco_person\"] = {\n",
    "    \"keypoints_coco_2014_train\": (\n",
    "        \"coco/train2014\",\n",
    "        \"coco/annotations/person_keypoints_train2014.json\",\n",
    "    ),\n",
    "    \"keypoints_coco_2014_val\": (\"coco/val2014\", \"coco/annotations/person_keypoints_val2014.json\"),\n",
    "    \"keypoints_coco_2014_minival\": (\n",
    "        \"coco/val2014\",\n",
    "        \"coco/annotations/person_keypoints_minival2014.json\",\n",
    "    ),\n",
    "    \"keypoints_coco_2014_valminusminival\": (\n",
    "        \"coco/val2014\",\n",
    "        \"coco/annotations/person_keypoints_valminusminival2014.json\",\n",
    "    ),\n",
    "    \"keypoints_coco_2014_minival_100\": (\n",
    "        \"coco/val2014\",\n",
    "        \"coco/annotations/person_keypoints_minival2014_100.json\",\n",
    "    ),\n",
    "    \"keypoints_coco_2017_train\": (\n",
    "        \"coco/train2017\",\n",
    "        \"coco/annotations/person_keypoints_train2017.json\",\n",
    "    ),\n",
    "    \"keypoints_coco_2017_val\": (\"coco/val2017\", \"coco/annotations/person_keypoints_val2017.json\"),\n",
    "    \"keypoints_coco_2017_val_100\": (\n",
    "        \"coco/val2017\",\n",
    "        \"coco/annotations/person_keypoints_val2017_100.json\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "_PREDEFINED_SPLITS_COCO_PANOPTIC = {\n",
    "    \"coco_2017_train_panoptic\": (\n",
    "        # This is the original panoptic annotation directory\n",
    "        \"coco/panoptic_train2017\",\n",
    "        \"coco/annotations/panoptic_train2017.json\",\n",
    "        # This directory contains semantic annotations that are\n",
    "        # converted from panoptic annotations.\n",
    "        # It is used by PanopticFPN.\n",
    "        # You can use the script at detectron2/datasets/prepare_panoptic_fpn.py\n",
    "        # to create these directories.\n",
    "        \"coco/panoptic_stuff_train2017\",\n",
    "    ),\n",
    "    \"coco_2017_val_panoptic\": (\n",
    "        \"coco/panoptic_val2017\",\n",
    "        \"coco/annotations/panoptic_val2017.json\",\n",
    "        \"coco/panoptic_stuff_val2017\",\n",
    "    ),\n",
    "    \"coco_2017_val_100_panoptic\": (\n",
    "        \"coco/panoptic_val2017_100\",\n",
    "        \"coco/annotations/panoptic_val2017_100.json\",\n",
    "        \"coco/panoptic_stuff_val2017_100\",\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "def register_all_coco(root):\n",
    "    for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_COCO.items():\n",
    "        for key, (image_root, json_file) in splits_per_dataset.items():\n",
    "            # Assume pre-defined datasets live in `./datasets`.\n",
    "            register_coco_instances(\n",
    "                key,\n",
    "                _get_builtin_metadata(dataset_name),\n",
    "                os.path.join(root, json_file) if \"://\" not in json_file else json_file,\n",
    "                os.path.join(root, image_root),\n",
    "            )\n",
    "\n",
    "    for (\n",
    "        prefix,\n",
    "        (panoptic_root, panoptic_json, semantic_root),\n",
    "    ) in _PREDEFINED_SPLITS_COCO_PANOPTIC.items():\n",
    "        prefix_instances = prefix[: -len(\"_panoptic\")]\n",
    "        instances_meta = MetadataCatalog.get(prefix_instances)\n",
    "        image_root, instances_json = instances_meta.image_root, instances_meta.json_file\n",
    "        # The \"separated\" version of COCO panoptic segmentation dataset,\n",
    "        # e.g. used by Panoptic FPN\n",
    "        register_coco_panoptic_separated(\n",
    "            prefix,\n",
    "            _get_builtin_metadata(\"coco_panoptic_separated\"),\n",
    "            image_root,\n",
    "            os.path.join(root, panoptic_root),\n",
    "            os.path.join(root, panoptic_json),\n",
    "            os.path.join(root, semantic_root),\n",
    "            instances_json,\n",
    "        )\n",
    "        # The \"standard\" version of COCO panoptic segmentation dataset,\n",
    "        # e.g. used by Panoptic-DeepLab\n",
    "        register_coco_panoptic(\n",
    "            prefix,\n",
    "            _get_builtin_metadata(\"coco_panoptic_standard\"),\n",
    "            image_root,\n",
    "            os.path.join(root, panoptic_root),\n",
    "            os.path.join(root, panoptic_json),\n",
    "            instances_json,\n",
    "        )\n",
    "\n",
    "\n",
    "# ==== Predefined datasets and splits for LVIS ==========\n",
    "\n",
    "\n",
    "_PREDEFINED_SPLITS_LVIS = {\n",
    "    \"lvis_v1\": {\n",
    "        \"lvis_v1_train\": (\"coco/\", \"lvis/lvis_v1_train.json\"),\n",
    "        \"lvis_v1_val\": (\"coco/\", \"lvis/lvis_v1_val.json\"),\n",
    "        \"lvis_v1_test_dev\": (\"coco/\", \"lvis/lvis_v1_image_info_test_dev.json\"),\n",
    "        \"lvis_v1_test_challenge\": (\"coco/\", \"lvis/lvis_v1_image_info_test_challenge.json\"),\n",
    "    },\n",
    "    \"lvis_v0.5\": {\n",
    "        \"lvis_v0.5_train\": (\"coco/\", \"lvis/lvis_v0.5_train.json\"),\n",
    "        \"lvis_v0.5_val\": (\"coco/\", \"lvis/lvis_v0.5_val.json\"),\n",
    "        \"lvis_v0.5_val_rand_100\": (\"coco/\", \"lvis/lvis_v0.5_val_rand_100.json\"),\n",
    "        \"lvis_v0.5_test\": (\"coco/\", \"lvis/lvis_v0.5_image_info_test.json\"),\n",
    "    },\n",
    "    \"lvis_v0.5_cocofied\": {\n",
    "        \"lvis_v0.5_train_cocofied\": (\"coco/\", \"lvis/lvis_v0.5_train_cocofied.json\"),\n",
    "        \"lvis_v0.5_val_cocofied\": (\"coco/\", \"lvis/lvis_v0.5_val_cocofied.json\"),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def register_all_lvis(root):\n",
    "    for dataset_name, splits_per_dataset in _PREDEFINED_SPLITS_LVIS.items():\n",
    "        for key, (image_root, json_file) in splits_per_dataset.items():\n",
    "            register_lvis_instances(\n",
    "                key,\n",
    "                get_lvis_instances_meta(dataset_name),\n",
    "                os.path.join(root, json_file) if \"://\" not in json_file else json_file,\n",
    "                os.path.join(root, image_root),\n",
    "            )\n",
    "\n",
    "\n",
    "# ==== Predefined splits for raw cityscapes images ===========\n",
    "_RAW_CITYSCAPES_SPLITS = {\n",
    "    \"cityscapes_fine_{task}_train\": (\"cityscapes/leftImg8bit/train/\", \"cityscapes/gtFine/train/\"),\n",
    "    \"cityscapes_fine_{task}_val\": (\"cityscapes/leftImg8bit/val/\", \"cityscapes/gtFine/val/\"),\n",
    "    \"cityscapes_fine_{task}_test\": (\"cityscapes/leftImg8bit/test/\", \"cityscapes/gtFine/test/\"),\n",
    "}\n",
    "\n",
    "\n",
    "def register_all_cityscapes(root):\n",
    "    for key, (image_dir, gt_dir) in _RAW_CITYSCAPES_SPLITS.items():\n",
    "        meta = _get_builtin_metadata(\"cityscapes\")\n",
    "        image_dir = os.path.join(root, image_dir)\n",
    "        gt_dir = os.path.join(root, gt_dir)\n",
    "\n",
    "        inst_key = key.format(task=\"instance_seg\")\n",
    "        DatasetCatalog.register(\n",
    "            inst_key,\n",
    "            lambda x=image_dir, y=gt_dir: load_cityscapes_instances(\n",
    "                x, y, from_json=True, to_polygons=True\n",
    "            ),\n",
    "        )\n",
    "        MetadataCatalog.get(inst_key).set(\n",
    "            image_dir=image_dir, gt_dir=gt_dir, evaluator_type=\"cityscapes_instance\", **meta\n",
    "        )\n",
    "\n",
    "        sem_key = key.format(task=\"sem_seg\")\n",
    "        DatasetCatalog.register(\n",
    "            sem_key, lambda x=image_dir, y=gt_dir: load_cityscapes_semantic(x, y)\n",
    "        )\n",
    "        MetadataCatalog.get(sem_key).set(\n",
    "            image_dir=image_dir,\n",
    "            gt_dir=gt_dir,\n",
    "            evaluator_type=\"cityscapes_sem_seg\",\n",
    "            ignore_label=255,\n",
    "            **meta,\n",
    "        )\n",
    "\n",
    "\n",
    "# ==== Predefined splits for PASCAL VOC ===========\n",
    "def register_all_pascal_voc(root):\n",
    "    SPLITS = [\n",
    "        (\"voc_2007_trainval\", \"VOC2007\", \"trainval\"),\n",
    "        (\"voc_2007_train\", \"VOC2007\", \"train\"),\n",
    "        (\"voc_2007_val\", \"VOC2007\", \"val\"),\n",
    "        (\"voc_2007_test\", \"VOC2007\", \"test\"),\n",
    "        (\"voc_2012_trainval\", \"VOC2012\", \"trainval\"),\n",
    "        (\"voc_2012_train\", \"VOC2012\", \"train\"),\n",
    "        (\"voc_2012_val\", \"VOC2012\", \"val\"),\n",
    "    ]\n",
    "    for name, dirname, split in SPLITS:\n",
    "        year = 2007 if \"2007\" in name else 2012\n",
    "        register_pascal_voc(name, os.path.join(root, dirname), split, year)\n",
    "        MetadataCatalog.get(name).evaluator_type = \"pascal_voc\"\n",
    "\n",
    "\n",
    "def register_all_ade20k(root):\n",
    "    root = os.path.join(root, \"ADEChallengeData2016\")\n",
    "    for name, dirname in [(\"train\", \"training\"), (\"val\", \"validation\")]:\n",
    "        image_dir = os.path.join(root, \"images\", dirname)\n",
    "        gt_dir = os.path.join(root, \"annotations_detectron2\", dirname)\n",
    "        name = f\"ade20k_sem_seg_{name}\"\n",
    "        DatasetCatalog.register(\n",
    "            name, lambda x=image_dir, y=gt_dir: load_sem_seg(y, x, gt_ext=\"png\", image_ext=\"jpg\")\n",
    "        )\n",
    "        MetadataCatalog.get(name).set(\n",
    "            stuff_classes=ADE20K_SEM_SEG_CATEGORIES[:],\n",
    "            image_root=image_dir,\n",
    "            sem_seg_root=gt_dir,\n",
    "            evaluator_type=\"sem_seg\",\n",
    "            ignore_label=255,\n",
    "        )\n",
    "\n",
    "def register_my_coco_datasets():\n",
    "    \"\"\"The function to register custom coco datasets.\"\"\"\n",
    "    register_coco_instances(\n",
    "        \"nfl2021_train\",\n",
    "        {},\n",
    "        \"/kaggle/input/nfl2021-coco-train-val-annotations/train.json\",\n",
    "        \"/kaggle/input/nfl2021-train-images/train_images\"\n",
    "    )\n",
    "    register_coco_instances(\n",
    "        \"nfl2021_valid\",\n",
    "        {},\n",
    "        \"/kaggle/input/nfl2021-coco-train-val-annotations/valid.json\",\n",
    "        \"/kaggle/input/nfl2021-train-images/train_images\"\n",
    "    )\n",
    "\n",
    "# True for open source;\n",
    "# Internally at fb, we register them elsewhere\n",
    "if __name__.endswith(\".builtin\"):\n",
    "    # Assume pre-defined datasets live in `./datasets`.\n",
    "    _root = os.getenv(\"DETECTRON2_DATASETS\", \"datasets\")\n",
    "    register_all_coco(_root)\n",
    "    register_all_lvis(_root)\n",
    "    register_all_cityscapes(_root)\n",
    "    register_all_cityscapes_panoptic(_root)\n",
    "    register_all_pascal_voc(_root)\n",
    "    register_all_ade20k(_root)\n",
    "    # Custom registeration happends here.\n",
    "    register_my_coco_datasets()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile configs/custom_faster_rcnn.yaml\n",
    "\n",
    "MODEL:\n",
    "  WEIGHTS: \"https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_1x/137257794/model_final_b275ba.pkl\"\n",
    "  MASK_ON: False\n",
    "  RESNETS:\n",
    "    DEPTH: 50\n",
    "  META_ARCHITECTURE: \"GeneralizedRCNN\"\n",
    "  BACKBONE:\n",
    "    NAME: \"build_resnet_fpn_backbone\"\n",
    "  RESNETS:\n",
    "    OUT_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "  FPN:\n",
    "    IN_FEATURES: [\"res2\", \"res3\", \"res4\", \"res5\"]\n",
    "  ANCHOR_GENERATOR:\n",
    "    SIZES: [[32], [64], [128], [256], [512]]  # One size for each in feature map\n",
    "    ASPECT_RATIOS: [[0.5, 1.0, 2.0]]  # Three aspect ratios (same for all in feature maps)\n",
    "  RPN:\n",
    "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\", \"p6\"]\n",
    "    PRE_NMS_TOPK_TRAIN: 2000  # Per FPN level\n",
    "    PRE_NMS_TOPK_TEST: 1000  # Per FPN level\n",
    "    # Detectron1 uses 2000 proposals per-batch,\n",
    "    # (See \"modeling/rpn/rpn_outputs.py\" for details of this legacy issue)\n",
    "    # which is approximately 1000 proposals per-image since the default batch size for FPN is 2.\n",
    "    POST_NMS_TOPK_TRAIN: 1000\n",
    "    POST_NMS_TOPK_TEST: 1000\n",
    "  ROI_HEADS:\n",
    "    NAME: \"StandardROIHeads\"\n",
    "    IN_FEATURES: [\"p2\", \"p3\", \"p4\", \"p5\"]\n",
    "    NUM_CLASSES: 1 # Added\n",
    "  ROI_BOX_HEAD:\n",
    "    NAME: \"FastRCNNConvFCHead\"\n",
    "    NUM_FC: 2\n",
    "    POOLER_RESOLUTION: 7\n",
    "DATASETS:\n",
    "  TRAIN: (\"nfl2021_train\",) # Modified\n",
    "  TEST: (\"nfl2021_valid\",) # Modified\n",
    "SOLVER:\n",
    "  IMS_PER_BATCH: 2 # Modified\n",
    "  BASE_LR: 0.002 # Modified\n",
    "  STEPS: (80000,) # If you changed MAX_ITER, it's better to change when you decrease LR too here.\n",
    "  MAX_ITER: 90000 # You may want to modify this to control how long to train\n",
    "INPUT:\n",
    "  MIN_SIZE_TRAIN: (640, 672, 704, 736, 768, 800)\n",
    "VERSION: 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing library \n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import numpy as np\n",
    "import os, json, cv2, random\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your dataset is in COCO format, this cell can be replaced by the following three lines:\n",
    "# from detectron2.data.datasets import register_coco_instances\n",
    "# register_coco_instances(\"my_dataset_train\", {}, \"json_annotation_train.json\", \"path/to/image/dir\")\n",
    "# register_coco_instances(\"my_dataset_val\", {}, \"json_annotation_val.json\", \"path/to/image/dir\")\n",
    "\"\"\"\n",
    "from detectron2.structures import BoxMode\n",
    "from detectron2.data import MetadataCatalog\n",
    "\n",
    "def get_data_dicts(img_dir):\n",
    "    json_file = os.path.join(img_dir, \"train.json\")\n",
    "    with open(json_file) as f:\n",
    "        imgs_anns = json.load(f)\n",
    "\n",
    "    dataset_dicts = []\n",
    "    for idx, v in enumerate(imgs_anns.values()):\n",
    "        record = {}\n",
    "        \n",
    "        filename = os.path.join(img_dir, v[\"filename\"])\n",
    "        height, width = cv2.imread(filename).shape[:2]\n",
    "        \n",
    "        record[\"file_name\"] = filename\n",
    "        record[\"image_id\"] = idx\n",
    "        record[\"height\"] = height\n",
    "        record[\"width\"] = width\n",
    "      \n",
    "        annos = v[\"regions\"]\n",
    "        objs = []\n",
    "        for _, anno in annos.items():\n",
    "            assert not anno[\"region_attributes\"]\n",
    "            anno = anno[\"shape_attributes\"]\n",
    "            px = anno[\"all_points_x\"]\n",
    "            py = anno[\"all_points_y\"]\n",
    "            poly = [(x + 0.5, y + 0.5) for x, y in zip(px, py)]\n",
    "            poly = [p for x in poly for p in x]\n",
    "\n",
    "            obj = {\n",
    "                \"bbox\": [np.min(px), np.min(py), np.max(px), np.max(py)],\n",
    "                \"bbox_mode\": BoxMode.XYXY_ABS,\n",
    "                \"segmentation\": [poly],\n",
    "                \"category_id\": 0,\n",
    "            }\n",
    "            objs.append(obj)\n",
    "        record[\"annotations\"] = objs\n",
    "        dataset_dicts.append(record)\n",
    "    return dataset_dicts\n",
    "\n",
    "for d in [\"train\", \"valid\"]:\n",
    "    DatasetCatalog.register(\"helmet_\" + d, lambda d=d: get_data_dicts(\"data/\" + d))\n",
    "    MetadataCatalog.get(\"helmet_\" + d).set(thing_classes=[\"helmet\"])\n",
    "training_metadata = MetadataCatalog.get(\"helmet\")\n",
    "\n",
    "\n",
    "\n",
    "#Data visulization \n",
    "\n",
    "\n",
    "dataset_dicts = get_data_dicts(\"../input/nfl-health-and-safety-helmet-assignment/images\")\n",
    "for d in random.sample(dataset_dicts, 3):\n",
    "    img = cv2.imread(d[\"file_name\"])\n",
    "    training_metadata = MetadataCatalog.get(d[\"file_name\"])\n",
    "    visualizer = Visualizer(img[:, :, ::-1], training_metadata, scale=0.5)\n",
    "    out = visualizer.draw_dataset_dict(d)\n",
    "    cv2_imshow(out.get_image()[:, :, ::-1])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"../configs/custom_faster_rcnn.yaml\"))\n",
    "cfg.DATASETS.TEST = ()\n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "cfg.SOLVER.BASE_LR = 0.00025  # pick a good LR\n",
    "cfg.SOLVER.MAX_ITER = 300    # 300 iterations seems good enough for this toy dataset; you will need to train longer for a practical dataset\n",
    "cfg.SOLVER.STEPS = []        # do not decay learning rate\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128   # faster, and good enough for this toy dataset (default: 512)\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1  # only has one class (ballon). (see https://detectron2.readthedocs.io/tutorials/datasets.html#update-the-config-for-new-datasets)\n",
    "# NOTE: this config means the number of classes, but a few popular unofficial tutorials incorrect uses num_classes+1 here.\n",
    "\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False)\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
